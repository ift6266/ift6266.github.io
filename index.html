
 <!DOCTYPE HTML>
<html lang="en-US">
<head>
  <meta charset="UTF-8">
  
    <title>Blog IFT6266 H16</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="Yifan Nie">
    

    
    <meta name="description" content="Blog for IFT6266 H16">
<meta property="og:type" content="website">
<meta property="og:title" content="Blog IFT6266 H16">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Blog IFT6266 H16">
<meta property="og:description" content="Blog for IFT6266 H16">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Blog IFT6266 H16">
<meta name="twitter:description" content="Blog for IFT6266 H16">

    
    <link rel="alternative" href="/atom.xml" title="Blog IFT6266 H16" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/jacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/jacman.jpg">
    
    <link rel="stylesheet" href="/css/style.css" type="text/css">
</head>

  <body>
    <header>
      
<div>
		
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="Blog IFT6266 H16">Blog IFT6266 H16</a></h1>
				<h2 class="blog-motto">by Yifan NIE</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="Menu">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
						<li><a href="/about">About</a></li>
					
					<li>
 					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="Search" />
						<input type="hidden" name="q" value="site:yoursite.com">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main">

   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2016/03/20/cats2.3/" title="Cats and Dogs 2.3 (6 Conv layer architecture) with rotation, valid_error 8.32%" itemprop="url">Cats and Dogs 2.3 (6 Conv layer architecture) with rotation, valid_error 8.32%</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Yifan Nie" target="_blank" itemprop="author">Yifan Nie</a>
		
  <p class="article-time">
    <time datetime="2016-03-20T13:28:52.000Z" itemprop="datePublished"> Published 2016-03-20</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>Hi,<br>As mentioned in Blog posts 2.2, this time, I tried to use a 6 layered CNN to conduct experiment, but with more feature maps at each layer, and at the last convolutional layer, there are 512 feature maps. More features maps might help the last MLP to better discriminate the image. </p>
<p>In this experiment 2.3, I still use the Random2DRotation Method to rotate each training image by a random degree, and keep their label untouched, to do regularization, so the model will learn different dogs or cats’images with different random rotated angles. However this time, I tried to set the initial learning rate for Adam() as 0.0005, rather than using its default values.</p>
<p>As we learned from course, initial learning rate is very important, if it is set big, the optimization will oscillate, and if it is set too big, the optimization process will diverge, it is linked with the inverse of the biggest eigenvalue of the Hessian of the cost function. However if the learning rate is chosen to be too small, the learning will be very slow, and here, as we are using Adam() to decay the learning rate as time elapses, at the end stage the learning rate will be very small can the learning cannot go on effectively.</p>
<p>And as tried in last experiment 2.2, I still use a learning rate of 0.0005 which seems to work well for this 6 layered architecture.</p>
<p>In this experiment, I’m using a 6-convolution layered CNN architecture.</p>
<p>The configurations are as follows:</p>
<pre><code>num_epochs= 63 (at the time of posting this blog)
batch_size=64
image_shape = (256,256)
filter_sizes = [(5,5),(5,5),(5,5),(5,5),(5,5),(5,5)]
feature_maps = [20,40,70,140,256, 512]
pooling_sizes = [(2,2),(2,2),(2,2),(2,2),(2,2),(2,2)]
mlp_hiddens = [1000]
output_size = 2
weights_init=Uniform(width=0.2)
step_rule=Adam(learning_rate=0.0005)
max_image_dim_limit_method= MaximumImageDimension
dataset_processing = rescale to 256*256
</code></pre><p>Validation error=8.32% after 63 epochs.</p>
<p>However we can see that at 63 epochs the validation error is only a little bit lower than the 8.5% of the last experiment which has 120 feature maps at the last layer, so the enhencement caused by adding feature maps seems not that much. It is worthwile to try other regularization techniques or better optimization method to help furthur improve performance in the next experiment.</p>
<p><img src="/image/2.3.png" alt="cats and dogs 2 result"></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


</div>




<div class="comments-count">
	
	  	<span></span>
		<a href="/2016/03/20/cats2.3/#disqus_thread" class="comments-count-link">Comments</a>
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2016/03/13/cats2.2/" title="Cats and Dogs 2.2 (6 Conv layer architecture) with rotation, valid_error 8.56%, choice of leanring rate is important" itemprop="url">Cats and Dogs 2.2 (6 Conv layer architecture) with rotation, valid_error 8.56%, choice of leanring rate is important</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Yifan Nie" target="_blank" itemprop="author">Yifan Nie</a>
		
  <p class="article-time">
    <time datetime="2016-03-13T13:24:51.000Z" itemprop="datePublished"> Published 2016-03-13</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>Hi,<br>As mentioned in Blog posts 2.1, this time, I tried to use a 6 layered CNN to conduct experiment, I tried to reduce the filter sizes of the last 3 layers to (4,4), and still keep other configurations unchanged.</p>
<p>In this experiment 2.2, I still use the Random2DRotation Method to rotate each training image by a random degree, and keep their label untouched, to do regularization, so the model will learn different dogs or cats’images with different random rotated angles. However this time, I tried to set the initial learning rate for Adam() as 0.0005, rather than using its default values.</p>
<p>As we learned from course, initial learning rate is very important, if it is set big, the optimization will oscillate, and if it is set too big, the optimization process will diverge, it is linked with the inverse of the biggest eigenvalue of the Hessian of the cost function. However if the learning rate is chosen to be too small, the learning will be very slow, and here, as we are using Adam() to decay the learning rate as time elapses, at the end stage the learning rate will be very small can the learning cannot go on effectively.</p>
<p>Also, the initial learning rate setting is important, because at the begining, we want to get down as fast as possible, so we cannot set too conservatively small learning rate. I’ve tried with trial and errors and concluded that 0.004 was too big, and the default 0.0002 is too small, this time I tried with 0.0005. It seemed work better!</p>
<p>In this experiment, I’m using a 6-convolution layered CNN architecture.<br>The configurations are as follows:</p>
<pre><code>num_epochs= 61 (at the time of posting this blog)
batch_size=64
image_shape = (256,256)
filter_sizes = [(5,5),(5,5),(5,5),(5,5),(5,5),(5,5)]
feature_maps = [20,40,60,80,100,120]
pooling_sizes = [(2,2),(2,2),(2,2),(2,2),(2,2),(2,2)]
mlp_hiddens = [1000]
output_size = 2
weights_init=Uniform(width=0.2)
step_rule=Adam(learning_rate=0.0005)
max_image_dim_limit_method= MaximumImageDimension
dataset_processing = rescale to 256*256
</code></pre><p>Validation error=8.56% after 61 epochs</p>
<p><img src="/image/2.2.png" alt="cats and dogs 2 result"></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


</div>




<div class="comments-count">
	
	  	<span></span>
		<a href="/2016/03/13/cats2.2/#disqus_thread" class="comments-count-link">Comments</a>
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2016/03/03/cats2.1/" title="Cats and Dogs 2.1 (6 Conv layer architecture) with rotation" itemprop="url">Cats and Dogs 2.1 (6 Conv layer architecture) with rotation</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Yifan Nie" target="_blank" itemprop="author">Yifan Nie</a>
		
  <p class="article-time">
    <time datetime="2016-03-03T14:24:51.000Z" itemprop="datePublished"> Published 2016-03-03</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>Hi,<br>As mentioned in Blog posts 2.01, this time, in 2.1 I tried to use a 6 layered CNN to conduct experiment, but I encountered the same strange thing as encountered by Florian. I used Adam() as update rule to do learning. During training the training error and validation error suddenly diverged and totally ruined the training process.</p>
<p>In this experiment 2.1, I still use the Random2DRotation Method to rotate each training image by a random degree, and keep their label untouched, to do regularization, so the model will learn different dogs or cats’images with different random rotated angles. I wil try to set the initial parameters for Adam() rather than using its default values.</p>
<p>In this experiment, I’m using a 6-convolution layered CNN architecture.<br>The configurations are as follows:</p>
<pre><code>num_epochs= 120 
image_shape = (256,256)
filter_sizes = [(5,5),(5,5),(5,5),(5,5),(5,5),(5,5)]
feature_maps = [20,40,60,80,100, 120]
pooling_sizes = [(2,2),(2,2),(2,2)]
mlp_hiddens = [1000]
output_size = 2
weights_init=Uniform(width=0.2)
step_rule=Adam()
max_image_dim_limit_method= MaximumImageDimension
dataset_processing = rescale to 256*256
</code></pre><p><img src="/image/2.1.png" alt="cats and dogs 2 result"></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


</div>




<div class="comments-count">
	
	  	<span></span>
		<a href="/2016/03/03/cats2.1/#disqus_thread" class="comments-count-link">Comments</a>
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2016/03/03/cats2.01/" title="Cats and Dogs 2.01 (5 Conv layer architecture) with rotatation , validation error 19.6%" itemprop="url">Cats and Dogs 2.01 (5 Conv layer architecture) with rotatation , validation error 19.6%</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Yifan Nie" target="_blank" itemprop="author">Yifan Nie</a>
		
  <p class="article-time">
    <time datetime="2016-03-03T13:24:51.000Z" itemprop="datePublished"> Published 2016-03-03</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>Hi,<br>As mentioned in Blog posts 1.3, I will try some deeper achitectures, so in this experiment 2.01 I tried to use a 5 layered CNN, and the configuration is showed as below.</p>
<p>So in order to overcome the overfitting phenomenon, there are a lot of methods to regularize, in this experiment 2.01, I still use the Random2DRotation Method to rotate each training image by a random degree, and keep their label untouched, so the model will learn different dogs or cats’images with different random rotated angles, and as we can see from the learning curve of experiment 2.01, this does help to reduce the overfitting. Globally, the training error and validation error curves are sticked together, and the validation error after 100 epochs is 19.6%. </p>
<p>In this experiment, I’m using the same 5-convolution layered CNN architecture.<br>The configurations are as follows:</p>
<pre><code>num_epochs= 100 
image_shape = (128,128)
filter_sizes = [(5,5),(5,5),(5,5),(5,5),(5,5)]
feature_maps = [20,40,60,80,100]
pooling_sizes = [(2,2),(2,2),(2,2),(2,2),(2,2)]
mlp_hiddens = [1000]
output_size = 2
weights_init=Uniform(width=0.2)
step_rule=Adam()
max_image_dim_limit_method= MaximumImageDimension
dataset_processing = rescale to 256*256
</code></pre><p><img src="/image/2.01.png" alt="cats and dogs 2 result"></p>
<p>But it seems a little bit strange because the validation error still stagnate at around 19.6%, should I train more epochs, or should I do other things to change the architecture, the processing of the images? </p>
<p>Next step I will try add a 6th conv layer and more feature maps at last conv layer to see if it can provide more information to the last MLP-softmax classification layer. And it might  also be worthwhile to try other data augmentation schemes to regularize.</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


</div>




<div class="comments-count">
	
	  	<span></span>
		<a href="/2016/03/03/cats2.01/#disqus_thread" class="comments-count-link">Comments</a>
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2016/02/28/cats1.3/" title="Cats and Dogs 1.3 (3 Conv layer architecture) with rotated image to regularize, validation error 19.4%" itemprop="url">Cats and Dogs 1.3 (3 Conv layer architecture) with rotated image to regularize, validation error 19.4%</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Yifan Nie" target="_blank" itemprop="author">Yifan Nie</a>
		
  <p class="article-time">
    <time datetime="2016-02-28T23:24:51.000Z" itemprop="datePublished"> Published 2016-02-28</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>Hi,<br>As mentioned in Blog posts 1.2, simply using the cropped image and limit their size to 128*128 doesn’t help a lot, and we can observe from experiment 1.2 that only use cropped original images to do learning will result in overfitting, and as we can observe from the learning curve in experiment 1.2, after about 700 epochs, overfitting already occurred, the valid_error will stagnate at around 27%, and the training error will continues to decrease to about 5%, but that isn’t of any use…</p>
<p>So in order to overcome the overfitting phenomenon, there are a lot of methods to regularize, in this experiment 1.3, I uses the Random2DRotation Method to rotate each training image by a random degree, and keep their label untouched, so the model will learn different dogs or cats’images with different random rotated angles, and as we can see from the learning curve of experiment 1.3, this does help to reduce the overfitting. Globally, the training error and validation error curves are sticked together, and the validation error after 100 epochs is 19.4%. </p>
<p>In this experiment, I’m using the same 3-convolution layered CNN architecture as in 1, 1.01, 1.1, 1.2<br>The configurations are as follows:</p>
<pre><code>num_epochs= 100 
image_shape = (128,128)
filter_sizes = [(5,5),(5,5),(5,5)]
feature_maps = [20,50,80]
pooling_sizes = [(2,2),(2,2),(2,2)]
mlp_hiddens = [1000]
output_size = 2
weights_init=Uniform(width=0.2)
step_rule=Adam()
max_image_dim_limit_method= MaximumImageDimension
dataset_processing = rescale to 128*128
</code></pre><p><img src="/image/cats1.3.png" alt="cats and dogs 1 result"></p>
<p>Next step I will try more complicated architectures (e.g. more layers) and with more feature maps at last conv layer to see if it can provide more information to the last MLP-softmax classification layer. And it might  also be worthwhile to try other data augmentation schemes to regularize.</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


</div>




<div class="comments-count">
	
	  	<span></span>
		<a href="/2016/02/28/cats1.3/#disqus_thread" class="comments-count-link">Comments</a>
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2016/02/27/cats1.2/" title="Cats and Dogs 1.2" itemprop="url">Cats and Dogs 1.2</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Yifan Nie" target="_blank" itemprop="author">Yifan Nie</a>
		
  <p class="article-time">
    <time datetime="2016-02-27T21:24:51.000Z" itemprop="datePublished"> Published 2016-02-27</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>Hi,<br>As mentioned in Blog posts 1.1, the fixed learning rate might not be a good choice, if it is chosen too small, the learning is to slow, and even cannot pull the cost function to a better minimum, on the other hand, if it is set too big, the loss will decrease and may oscillate and even bounces back to a higher loss as encountered in the figure of experiment 1.01… So Adam() might be a better choice.</p>
<p>In this experiment, I’m using the same 3-convolution layered CNN architecture as in 1 and 1.01<br>The configurations are as follows:</p>
<pre><code>num_epochs= 100 early stopped
image_shape = (128,128)
filter_sizes = [(5,5),(5,5),(5,5)]
feature_maps = [20,50,80]
pooling_sizes = [(2,2),(2,2),(2,2)]
mlp_hiddens = [1000]
output_size = 2
weights_init=Uniform(width=0.2)
step_rule=Adam()
max_image_dim_limit_method= MaximumImageDimension
dataset_processing = rescale to 128*128
</code></pre><p><img src="/image/main15.png" alt="cats and dogs 1 result"></p>
<p>This time, we can still observe the phenomenon of overfitting, so I just stopped training at epoch 21 because the valid error no longer decreases, and stagnate at around 25% however the training error still goes down. So this architecture setting might has its limitaitons. So data augmentation is very important to avoid things like overfitting. I’ll try to use rotations transformations to regularize.</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


</div>




<div class="comments-count">
	
	  	<span></span>
		<a href="/2016/02/27/cats1.2/#disqus_thread" class="comments-count-link">Comments</a>
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2016/02/27/cats1.1/" title="Cats and Dogs 1.1" itemprop="url">Cats and Dogs 1.1</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Yifan Nie" target="_blank" itemprop="author">Yifan Nie</a>
		
  <p class="article-time">
    <time datetime="2016-02-27T19:42:51.000Z" itemprop="datePublished"> Published 2016-02-27</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>Hi,<br>As mentioned in Blog posts 1 and 1.01, the random fixed size crop doesn’t seems to be a good way to limit the size of image to a fixed size (e.g.128*128), so inspired by Florian’s blog, I used a modified version of MinImageDimension to do MaxImageDimension to limit the size of image to a fixed size.<br>In this experiment, I’m using the same 3-convolution layered CNN architecture as in 1 and 1.01<br>The configurations are as follows:</p>
<pre><code>num_epochs= 100 early stopped
image_shape = (128,128)
filter_sizes = [(5,5),(5,5),(5,5)]
feature_maps = [20,50,80]
pooling_sizes = [(2,2),(2,2),(2,2)]
mlp_hiddens = [1000]
output_size = 2
weights_init=Uniform(width=0.2)
step_rule=Scale(learning_rate=0.05)
max_image_dim_limit_method= MaximumImageDimension
dataset_processing = rescale to 128*128
</code></pre><p><img src="/image/main14.png" alt="cats and dogs 1 result"></p>
<p>This time, we can observe the phenomenon of overfitting, so I just stopped training at epoch 27 because the valid error no longer decreases… Maybe I will try to use Adam() update rules rather than fixed learning rate, and do rotations for the images in order to reduce overfitting.</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


</div>




<div class="comments-count">
	
	  	<span></span>
		<a href="/2016/02/27/cats1.1/#disqus_thread" class="comments-count-link">Comments</a>
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2016/02/27/cats2/" title="Cats and Dogs 2 valid_err=19.91%" itemprop="url">Cats and Dogs 2 valid_err=19.91%</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Yifan Nie" target="_blank" itemprop="author">Yifan Nie</a>
		
  <p class="article-time">
    <time datetime="2016-02-27T19:26:51.000Z" itemprop="datePublished"> Published 2016-02-27</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>I also ran an experiment on the cluster, so I used a different architecture, So I called this experiment 2, however the image max size limiting method is still RandomFixedSizeCrop<br>In this experiment, I’m using a 3-convolution layered CNN<br>The configurations are as follows:</p>
<pre><code>num_epochs= 100 early stopped
image_shape = (128,128)
filter_sizes = [(5,5),(5,5),(5,5),(4,4),(4,4),(4,4)]
feature_maps = [20,40,60,80,100,120]
pooling_sizes = [(2,2),(2,2),(2,2),(2,2),(2,2),(2,2)]
mlp_hiddens = [1000]
output_size = 2
weights_init=Uniform(width=0.2)
step_rule=Adam()
max_image_dim_limit_method= RandomFixedSizeCrop
dataset_processing = rescale to 128*128
</code></pre><p>The result is after 100 epochs is that training error=19.57%, validation error=19.91%</p>
<h2 id="TRAINING_HAS_BEEN_FINISHED_3A"><a href="#TRAINING_HAS_BEEN_FINISHED_3A" class="headerlink" title="TRAINING HAS BEEN FINISHED:"></a>TRAINING HAS BEEN FINISHED:</h2><p>Training status:<br>batch_interrupt_received: False<br>epoch_interrupt_received: False<br>epoch_started: False<br>epochs_done: 100<br>iterations_done: 31300<br>received_first_batch: True<br>resumed_from: None<br>training_started: True<br>Log records from the iteration 31300:<br>saved_to: (‘catsVsDogs128.pkl’,)<br>time_read_data_this_epoch: 1.25491786003<br>time_read_data_total: 453.110922098<br>time_train_this_epoch: 163.532570601<br>time_train_total: 16410.0936284<br>train_cost: 0.408693790436<br>train_error_rate: 0.195686891675<br>train_total_gradient_norm: 4.36319255829<br>training_finish_requested: True<br>training_finished: True<br>valid_cost: 0.452142506838<br>valid_error_rate: 0.199169307947<br>valid_error_rate2: 0.199169307947</p>
<p>I kind of understand why the learning is kind of slow and why there’s no overfitting phenomen in 1.0 and 1.01 because I used RandomFixedSizeCrop to limit the max size of image to 128*128. This might crop the unimportant area area and label it as cats/dogs thus create some noises…<br>In next experiments, inspired by Florian’s approach, I will try to modify the MinimumImageDimension function so as to limit the maximum Image dimension.</p>
<p>The same datset pre-processing with more layers does help to boost performance, and the last convolution layer has 120 feature maps, which feeds more features to the MLP classifier.</p>
<p>In this experiment, I also used Adam() as learning rule, instead of fixed learning rate, this might also helps the performance to improve.</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


</div>




<div class="comments-count">
	
	  	<span></span>
		<a href="/2016/02/27/cats2/#disqus_thread" class="comments-count-link">Comments</a>
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2016/02/27/cats1.01/" title="Cats and Dogs 1.01" itemprop="url">Cats and Dogs 1.01</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Yifan Nie" target="_blank" itemprop="author">Yifan Nie</a>
		
  <p class="article-time">
    <time datetime="2016-02-27T18:04:51.000Z" itemprop="datePublished"> Published 2016-02-27</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>Hi,<br>I’ve tried another time the same configuration as in experiment 1, still with random initialization of the weights, just to add the Bokeh plotting, but the results seems to be worse than experiment 1<br>In this experiment, I’m using a 3-convolution layered CNN<br>The configurations are as follows:</p>
<pre><code>num_epochs= 100 
image_shape = (128,128)
filter_sizes = [(5,5),(5,5),(5,5)]
feature_maps = [20,50,80]
pooling_sizes = [(2,2),(2,2),(2,2)]
mlp_hiddens = [1000]
output_size = 2
weights_init=Uniform(width=0.2)
step_rule=Scale(learning_rate=0.1)
max_image_dim_limit_method= random crop
dataset_processing = rescale to 128*128
</code></pre><p>The result is after 100 epochs is that training error=35%, validation error= 35%</p>
<p><img src="/image/main11.png" alt="cats and dogs 1 result"></p>
<p>I kind of understand why the learning is kind of slow and why there’s no overfitting phenomen in 1.0 and 1.01 because I used RandomFixedSizeCrop to limit the max size of image to 128*128. This might crop the unimportant area area and label it as cats/dogs thus create some noises…<br>In next experiments, inspired by Florian’s approach, I will try to modify the MinimumImageDimension function so as to limit the maximum Image dimension.</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


</div>




<div class="comments-count">
	
	  	<span></span>
		<a href="/2016/02/27/cats1.01/#disqus_thread" class="comments-count-link">Comments</a>
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2016/02/26/cats1.0/" title="Cats and Dogs 1" itemprop="url">Cats and Dogs 1</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Yifan Nie" target="_blank" itemprop="author">Yifan Nie</a>
		
  <p class="article-time">
    <time datetime="2016-02-27T01:04:51.000Z" itemprop="datePublished"> Published 2016-02-26</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>Hi,<br>Just have a first try of cats and dogs project, the loading of the dataset does took me some time.<br>In this experiment, I’m using a 3-convolution layered CNN<br>The configurations are as follows:</p>
<pre><code>num_epochs= 100
image_shape = (128,128)
filter_sizes = [(5,5),(5,5),(5,5)]
feature_maps = [20,50,80]
pooling_sizes = [(2,2),(2,2),(2,2)]
mlp_hiddens = [1000]
output_size = 2
weights_init=Uniform(width=0.2)
step_rule=Scale(learning_rate=0.1)
Max_image_dim_limit_method = RandomFixedSizeCrop
Dataset_processing = rescale to 128*128
</code></pre><p>The result is after 100 epochs is that training error=26.31%, validation error= 25.77%</p>
<p>I still have some difficulty to plot the learning curves because of the Bokeh live plot problem, it always prompts me that it cannot import curstate module….</p>
<p>I tried to unpackage the pickled log file but pickle also throws error:<br>AttributeError : Unpickler instance has no attibute ‘persistent_load’. I’m trying to fix it and for the moment I have to post just a snapshot of the console snapshot.</p>
<p><img src="/image/cats1.jpg" alt="cats and dogs 1 result"></p>
<p>The learning is kind of slow, the training loss and training error decreases very slowly, I don’t know why, because the prof said using SDG will bring the loss/training error rate quickly down at first several epochs…</p>
<p>I’ll try to investigate this problem.</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


</div>




<div class="comments-count">
	
	  	<span></span>
		<a href="/2016/02/26/cats1.0/#disqus_thread" class="comments-count-link">Comments</a>
	
</div>

</footer>


    </article>







  <nav id="page-nav" class="clearfix">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next<span></span></a>
  </nav>

</div>
      <div class="openaside"><a class="navbutton" href="#" title="Show Sidebar"></a></div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="Hide Sidebar"></a></div>
<aside class="clearfix">

  


  

  

  <div class="linkslist">
  <p class="asidetitle">Links</p>
    <ul>
        
          <li>
            
            	<a href="http://diro.umontreal.ca" target="_blank" title="UdeM DIRO">UdeM DIRO</a>
            
          </li>
        
    </ul>
</div>

  


  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS</a>
</div>

</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<section class="info">
		<p> Hello, this is Yifan Nie&#39;s blog for IFT6266H16 <br/>
			</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		
		
		
		
		
		
		
		
		
	</div>
			
		

		<p class="copyright">
		Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/wuchong/jacman" target="_blank" title="Jacman">Jacman</a> © 2016 
		
		<a href="/about" target="_blank" title="Yifan Nie">Yifan Nie</a>
		
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
        
    }
  });
});
</script>










<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?e6d1f421bbc9962127a50488f9ed37d1";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>



<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="Back to Top"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->

<script type="text/x-mathjax-config"> 
MathJax.Hub.Config({ 
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} 
}); 
</script>
<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
  </body>
 </html>
